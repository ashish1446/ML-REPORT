\documentclass[12pt]{article}
%\renewcommand{\thesection}{}
\usepackage{mathtools}
\usepackage{commath}
\usepackage[sc,osf]{mathpazo}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
%\usepackage[plain]{algorithm}
%\usepackage{algorithmic}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

%\usepackage[noend]{algorithmic} 
\usepackage{algorithmic} 
\usepackage{algorithm,caption}
\algsetup{indent=2em} 
\renewcommand{\algorithmiccomment}[1]{\hspace{2em}// #1} 

\newtheorem*{thm}{Theorem}



\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}

\urlstyle{same}



\begin{document}
	
	   
	     
	
		\enlargethispage{2cm}
		
		\begin{center}
			
			\vspace*{-1cm}
			
			\textbf{\Large Notes on Machine Learning     }(incomplete)\\[10pt]
			

\textbf{\Large Aswin}\\ [8pt]			
			
			\end{center}
		
\cleardoublepage

\tableofcontents
\newpage

\section{Overview}

I made this document as a way to learn ML for my Maters' thesis . It’s not an original work,but a compilation of scribed notes of the ML course by Dr Sanjoy Dasgupta(UCSD), [SD] which I audited.Some parts are drawn directly/inspired from Andrew NG's Stanford CS229 course notes, [ANG] and Kilian weinberger's Cornell CS4780 course notes[KW].My primary reference was 'The Elements of Statistical Learning' by Trevor Hastie,Robert Tibshirani,Jerome Friedman, [HTF].So there will be considerable overlap with the aforementioned materials.This note might lack mathematical rigor but I have tried to give proofs and supplementary topics wherever necessary.For each algorithm, I have given a url to Github repo containing the implemetation using one of the three datasets viz Fisher's Iris dataset, Wisconsin Breast Cancer Dataset and MNIST handwritten digits dataset.
Please write to me if you find any inaccuracies.I hope this   proves at least moderately
interesting or useful for you.

\cleardoublepage

\section{Supervised Learning}

	In a typical scenario, we have an outcome measurement, usually quantitative (such as a stock price) or
categorical (such as heart attack/no heart attack), that we wish to predict
based on a set of features (such as diet and clinical measurements). We
have a training set of data, in which we observe the outcome and feature
measurements for a set of objects (such as people). Using this data we build
a prediction model, or learner, which will enable us to predict the outcome
for new unseen objects. A good learner is one that accurately predicts such
an outcome.
The examples above describe what is called the supervised learning problem. It is called “supervised” because of the presence of the outcome variable to guide the learning process.[HTF]

 


\subsection{Variable Types and Terminology}

The outcome measurement which we wish to predict denoted as \textbf{\textit{outputs}} depend on a set of variebles denoted as \textbf{\textit{inputs}}.
Classicaly,the \textit{inputs} are independent varibales whereas \textit{outputs} are dependent variables.
The term \textit{features} will be used interchangeably with inputs.

The \textit{outputs} which we wish to predict can be qualitative or quantitative(as in blood sugar level).
When the \textit{outputs} are qualitative (as in spams or  not spams),it is referred as categorical or discrete variables and are typically represented numerically by codes,as in -spam or not spam can be coded as -1 or 1.
Depending upon the kind of output varible, the prediction task can be of two types: \textit{regression} when we predict quantitative outputs and \textit{classification} when we predict  qualitative outputs.




 The input variables/features are denoted by $x^{(i)}$ and  the space of all such $x^{(i)}$ is $X$.
 The output variable that we are trying to predict is denoted as $y^{(i)}$ and the space of all such $y^{(i)}$ is $Y$.
 A pair $(x^{(i)},y^{(i)})$ is called a training example and the dataset, we will be using to learn - a collection of $n$ training examples $ \{(x^{(i)},y^{(i)});i=1,2,...,n\} $ - is called a training set.
 The superscript $“(i)”$ in the notation is simply an index into the training set.
 
 


\cleardoublepage

\section{Linear Regression}

Given a vector of inputs, $x^{T}=(x_{1},x_{2},..., x_{d})$.We need to know functions/hypotheses $h$ that can  approximate $y$ as a linear function of $x$: $$ h_{\theta}(x)=\theta_{0} + \sum_{i=1}^{d}x_{j}\theta_{j}$$
$\theta_{i}$s are parameters/weight parametrizing the space of linear functions mapping from $X$ to $Y$.The term $\theta_{0}$ is the intercept, also known as the \textit{bias} in machine learning.It is covenient to include $\theta_{0}$ in the vector of weights $\theta$  and add constant variable $1$ to the vector $x$, so that $$ h_{\theta}(x) = \theta^{T}x$$ $w$ ( weights) and $b$  (bias) can be interchangably used with $\theta$ and $\theta_{0}$ respectively.




For a training set,we have to  learn the parameters $\theta,$ so that we can predict $y$. One reasonable method seems to be to make $h(x)$ close to y,for the training examples.To formalize this, we will define a function that measures, for each value of the θ’s, how close the $h(x^{(i)})'$ s are to the corrsponding $y^{(i)}$'s.

We  define Cost/Loss function: $$J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^{2},$$this is the Least Squares cost function.In this approach, we pick the coefficients $\theta$ to minimize the cost function $J$.The LS cost function is quadratic function in weights, $\theta$ and hence it's minimum always exist, but may not be unique.
Given a set of training examples $(x^{(i)},y^{(i)})$ i.e training set ,define a matrix $X$ to be the $m$-by-$n$ matrix  that contains the input values of training examples in it's rows:  

 $$ \begin{bmatrix} 
  & {(x^{(1)})^{T}} &  \\
  & \vdots & \\
  &   (x^{(m)})^{T}     &  
  \end{bmatrix} $$
 
 
Let $ \textbf y$ be the $m$-dimensional vector containing the target/output values from the training set:
 $$\begin{bmatrix} 
 & y^{(1)} &  \\
 & \vdots & \\
 &   y^{(m)}     &  
 \end{bmatrix}$$
 
 
Since $h_{\theta}(x^{(i)})=(x^{(i)})^{T}\theta$,we can verify that
 $$X \theta - \textbf y = \begin{bmatrix} 
 & {(x^{(1)})^{T}}\theta &  \\
 & \vdots & \\
 &   (x^{(m)})^{T} \theta    &  
 \end{bmatrix} - \begin{bmatrix} 
 & y^{(1)} &  \\
 & \vdots & \\
 &   y^{(m)}     &  
 \end{bmatrix}$$
 
 $$=\begin{bmatrix} 
 & h_{\theta}(x^{(1)})-y^{(1)} &  \\
 & \vdots & \\
 &  h_{\theta}(x^{(m)})- y^{(m)}     &  
 \end{bmatrix}$$
 
 
 $\frac{1}{2}(X\theta-\textbf y)^{T}(X\theta-\textbf y)=\frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}=J(\theta)$ This is the cost function.To minimize $J$,we have to find the derivatives with respect to $\theta$
 
 
 
 
Some matrix derivative results
 $$\nabla_{A}tr AB = B^{T}$$ 
 $$\nabla_{A}\vert A\vert=\vert A\vert(A^{-1})^{T}$$ 
 $$\nabla_{A^{T}}f(A)= (\nabla_{A}f(A))^{T}$$ 
 $$\nabla_{A}trABA^{T}C=CAB+C^{T}AB^{T}$$
 
 
 Using the last two results 
 $$\nabla_{A^{T}}trABA^{T}C=B^{T}A^{T}C^{T}+BA^{T}C$$
 
 
 
 The cost function,  $J(\theta)=\frac{1}{2}(X\theta-\textbf{y})^{T}(X\theta-\textbf{y})$
 
 
$$ \nabla_{\theta} J(\theta)  =\nabla_{\theta}\frac{1}{2}(X\theta-\textbf{y})^{T}(X\theta-\textbf{y}) $$
 
 
 
  $$=\frac{1}{2}\nabla_{\theta}(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}\textbf{y}-\textbf{y}^{T}X\theta+\textbf{y}^{T}\textbf{y}) $$
 
 
 
$$ =\frac{1}{2}\nabla_{\theta} tr(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}\textbf{y}-\textbf{y}^{T}X\theta+\textbf{y}^{T}\textbf{y}) $$
 
 
 
 
$$=\frac{1}{2}\nabla_{\theta}(tr \theta^{T}X^{T}X\theta-2tr\textbf{y}^{T}X\theta)  $$
 
 

$$=\frac{1}{2}(X^{T}X\theta+X^{T}X\theta-2X^{T}\textbf{y})  =X^{T}X\theta-X^{T}\textbf{y} $$







To minimize $J,$ we set the derivative to zero and obtain:$X^{T}(X\theta-\textbf{y})=0$. If $X^{T}X$ is nonsingular then the value of $\theta $ that minimizes $J(\theta)$ is given in closed form by the equation,
$$\theta=(X^{T}X)^{-1}X^{T}\textbf{y}$$
Now that we have the parameters,we can predict the output corresponding to the input $x^{(i)}$ as $y^{(i)}=\theta^{T}x^{(i)}$



\section{The Perceptron}
In a binary classification problem , where dataset(D) is  $(x,y) \in \mathbb{R}^{d} \times\{-1,1\}$, the  learning problem is to find a hyperplane which separates the data into two classes, 
 assuming the data is linearly classifiable. The hyperplane is parametrized by $w\in\mathbb{R}^{d}$ and $ b \in \mathbb{R}$ such that $w.x + b = 0 $, so the poblem is equivalent to learning  the parameters $w$ and $b$.On point $x$, we predict the label as $\textbf{sign(w.x + b)}$.\\
$$(w.x + b) > 0  \implies y = +1 \therefore y(w.x + b) > 0 $$

$$(w.x + b) < 0  \implies y = -1 \therefore y(w.x + b) > 0 $$
 
i.e., If the true label of x is y, then $y(w.x + b) > 0 $, whereas for a misclassified pointi$y(w.x + b) \leq 0 $. The Loss function  for the perceptron can be defined as 





\begin{equation} 
Loss=
\begin{cases}
0, & \text{if} \ y(w.x + b) > 0 
 \\
-y(w.x + b) , & \text{if} \ y(w.x + b) \leq 0 
\end{cases}
\end{equation}

This loss function is a convex function of $y(w.x + b)$, and we can use stochastic gradient descent to find the value of  parameters that minimizes the loss function.
The update on the parameter w can be written as $w := w - \eta \nabla L(w)$,where $w = [w,b]$.The derivative of Loss function with respect to the parameters(assuming bias term is not absorbed into the weight vector) are 

$$\frac{\partial L}{\partial w} = -yx , \frac{\partial L}{\partial b} = -y$$.

So the update rule will be $w := w + \eta yx $ and $b := b + \eta y$. For $w = [w,b]$, this is equivalent  to  $w := w + \eta yx $.
If $\eta = 1$, then the update rule will be  $w := w +  yx $.

\begin{algorithm}
	
	\caption*{Perceptron Algorithm} \label{alg:MyAlgorithm}
	\begin{algorithmic}
		\STATE Initialize $\vec{w} = 0$
		
		\WHILE {TRUE }
		\STATE $m=0$
		\FOR{$(x_{i},y_{i}) \in D$}
		
		\IF {$y_{i}(\vec{w}^{T} . x_{i})\leq 0$}
		
		\STATE $\vec{w} \leftarrow \vec{w} + yx$
		\STATE $m \leftarrow m + 1$
		
		\ENDIF
		\ENDFOR
		
		\IF {$m = 0$}
		\STATE break
		
		\ENDIF
		
		
		
		\ENDWHILE
		
	\end{algorithmic}
\end{algorithm}


 \cleardoublepage

 If the taining data is linearly classifiable, Perceptron is guaranteed to converge after finite number of steps and return a seperating hyperplane with zero training error.
 

 
 \textbf{Margin $\gamma$ of a hyperplane} $w$ is defined as $\gamma = min_{(x_{i},y_{i})\in D}  \frac{ \vert x_{i}^{T}w \vert}{\|w\|_{2}}$, i.e it is the distance to the closest data point from the hyperplane paramtrized by $w$.
 
 \begin{thm}If a data set is linearly separable, the Perceptron will find a separating hyperplane in a finite number of updates.\end{thm}
 
 \begin{proof}
 	Suppose $\exists w^{*}$ such that $ y_{i}(x^{T}w^{*}) > 0$ $ \forall (x_{i},y_{i}) \in D$.Suppose that we rescale each data point and the $w^{*}$ such that
 	
 	
 	$$\|w^{*}\| = 1 \ and \ \|x_{i}\| \leq 1 \forall x_{i} \in D $$
 	
    So the margin $\gamma$ for the hyperplane $w^{*}$ becomes $ \gamma = min_{(x_{i},y_{i})\in D} \vert x_{i}^{T}w^{*} \vert $. After rescaling, all inputs   $x_{i}$ lies in a unit sphere in d-dimensional space.The separating hyperplane is defined  by $w^{*}$ with $\|w\|^{*} = 1$ i.e is \textbf{$w^{*}$} lies exactly on the unit sphere.
    
    We claim that if the above assumptions hold,then the Perceptron algorithm makes atmost $\frac{1}{\gamma ^{2}}$ mistakes.The update on w is only done in the instance of misclassification, i.e when  $y(x^{T}w) \leq 0$ holds.As $w^{*}$ is a seperating hyper-plane and classifies all points correctly,  $y(x^{T}w)>0$ $ \forall x$.
    
     Consider the effect of an update $w \leftarrow w + xy $ on the two terms $w^{T}w^{*}$ and $w^{T}w$.
     
     $w^{T}w^{*} = (w+xy)^{T}w^{*} = w^{T}w^{*} + y(x^{T}w^{*}) \geq w^{T}w^{*} + \gamma $
        
    The inequality follows from the fact that, for $w^{*}$, the distance from the hyperplane defined by $w^{*}$ to $x$ must be at least $\gamma$ i.e
    $y(x^{T}w^{*}) = \vert x^{T}w^{*}\vert \geq \gamma$.This implies that with each update $w^{T}w^{*}$ grows atleast by $\gamma$
 
     $w^{T}w = (w+xy)^{T}(w+xy) = w^{T}w + \underbrace{2y(w^{T}x)}_\text{$ \leq 0$} + \underbrace{y^{2}(x^{T}x)}_\text{$0 \leq \ \leq 1$}  \leq
     w^{T}w + 1$.
     This inequality follows the fact that, $2y(w^{T}x) \leq 0$,  as we had to make an update, meaning $x$ was misclassified. $0 \leq y^{2}(x^{T}x) \leq 1$ as $y^{2}=1 $ always and all $x^{T}x \leq 1$ as $\|x\| \leq 1$,(rescaled). This implies that $w^{T}w$ grows at most by 1.
     
     
     \cleardoublepage
     
     After $M$ updates, the two inequalities becomes $$w^{T}w^{*} \geq M\gamma$$  $$w^{T}w \leq M$$
 
 $$M\gamma \leq w^{T}w^{*} \leq \vert w^{T}w^{*} \vert \leq \| w^{T} \| \underbrace{\|w^{*}\|}_\text{1}=\sqrt{w^{T}w}$$
 
 $w^{T}w$ can most be $M$, as $w$ is initialized with 0 and with each update $w^{T}w$ grows at most by 1.
 
 $$\implies M\gamma \leq \sqrt{M} $$
 $$\implies M^{2}\gamma^{2} \leq M$$
 $$\implies M \leq \frac{1}{\gamma ^{2}}$$  Hence the number of updates $M$ is bounded from above by a constant.
  
 
 %	\underbrace{(x + 2)^3}_\text{text 1}
 	
 \end{proof}
 
 \section{Boosting}
 \textbf A {Weak Classifier} is the one with accuracy marginally better than random guessing. For a binary  weak classifier this means, $P(h(x) \neq y) = \frac{1}{2} - \epsilon .$ An learning algorithm which consistently generate such weak classifier is called a \textbf{weak learner}.\\ \textbf{Boosting} is a machine learning approach where such weak learners are combined to get better prediction acuracy.\\


\cleardoublepage

 \subsection{AdaBoost}
 
 
 \begin{algorithm}
 	
 	\caption*{AdaBoost Algorithm} \label{alg:MyAlgorithm}
 	\begin{algorithmic}
 		
 		\STATE 1. \ Given $(x^{(i)},y^{(i)}),...,(x^{(N)},y^{(N)}),$ where $y^{(i)}\in \{-1,+1\}$ 
 		\STATE 2. \  Initialize the observation weights $w_{i}= \frac{1}{N},i = 1,2,...,N.$
 		
 		\STATE 3. \ For m =1 to M:
 		
 		
 		
 		\STATE \hspace{ 0.8cm}(a) \ Fit a classifier $h_{m}(x)$ to the trainng data using weights $w_{i}.$
 		
 		\STATE \hspace{ 0.8cm}(b) Compute 
 		
 	
 		
 			$$err_{m} = \frac{\sum_{i=1}^{N}w_{i}y^{(i)}h_{m}(x^{(i)})}{\sum_{i=1}^{N} w_{i}}$$
 		
 		
 		
 			\STATE \hspace{ 0.8cm}(c) compute $\alpha_{m} = \log((1-err_{m})/err_{m})$
 			
 				\STATE \hspace{ 0.8cm}(d) set $w_{i} \leftarrow w_{i}.exp[-\alpha_{m}.y^{(i)}h_{m}(x^{(i)})], \ i = 1,2,...,N$
 				
 	\STATE 4. \ Output $H(x) = sign[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)]$
 		
 		
 	\end{algorithmic}
 \end{algorithm}
 
 
 
 
 
 \section{Nearest Neighbour Methods}
 
 Nearest-neighbour methods use those observations in the taining set $\mathcal{T}$ closest in input space to $x$ to form $\hat{Y}$.
 The k-nearest neighbour fit for $\hat{Y}$ is defined as follows:
 
 $$\hat{Y(x)} = \frac{1}{k}\sum_{x_{i} \in N_{k}(x)} y_{i},$$ where $N_{k}$ is neighbourhood of $x$ defined by $k$ closest points $x_{i}$ in the training sample.
 
 Closeness is quantified by a metric, which for instance can be assumed as Euclidean distance.We find $k$ observations with $x_{i}$ closes to x in the input space of training set, and average their responses.
 
 The notion of distance between two vectors is defined by a norm. A norm is a function from a vector space over the real or complex numbers to the nonnegative real numbers that satisfies certain properties pertaining to scalability and additivity, and takes the value zero if only the input vector is zero.
 
 As mentioned above we will use the Euclidean norm for all practical purposes.The Euclidean norm  is a specific norm on a vector space, that is strongly related with the Euclidean distance, and equals the square root of the inner product of a vector with itself.On an n-dimensional Euclidean space $ R^{n}$, the intuitive notion of length of the vector $x = (x1, x2, ..., xn)$ is captured by the formula
 
 $$\vert \vert x \vert \vert _{2} = \sqrt{x_{1}^{2}+ \dots x_{n}^{2}}$$
 
 
This definition gives distance between two vectors as the euclidean norm of the component wise difference i.e if $x,y \in R^{n}$ then $ \Vert\left(x-y)\vert\right\vert_{2} = \sqrt{\sum\limits_{i=1}^{n}(x_{i}-y_{i})^{2}} $
 
 
 
 
 
 
 
 \section{Support Vector Machines \tiny{[HTF][KW]}} 
 
 The Perceptron algorithm assures to return a linear seperating hyperplane if one exists.Existence of one  such  hyperplane implies that there is infinite such hyperplanes, and the perceptron doesn't guarantee to return the optimal seperating hyperplane i.e, the one with maximum margin.
 Support vector machines(SVM) finds maximum margin hyperplane. 
 
 The training dataset is $(x^{(i)}, y^{(i)}) \in \mathbb{R}^{d} \{-1,+1\},i = 1,...,N$ We define a  hyperplane by $\mathcal{H}=\{x \vert w^{T}x + b =0\}$ paramtrized by w and b.Let the margin $\gamma$ be defined as the distance from the hyperplane to the closest point across both the classes. A classifiction rule induced by the hyperplane is $h(x) = sign(w^{T}x+ b)$
 
 
 Consider some point $x$. Let $d$ be the vector from $\mathcal{H}$ to $x$ of minimum length. Let $x^{P}$ be the projection of $x$ onto $\mathcal{H}$. It follows then that:
 
 $x^{P} = x - d.$
 $w$ is perpendicular to $\mathcal{H}$. $d$ is parallel to $w$,  so $d = \alpha w$ for some $\alpha \in \mathbb{R}$. $x^{P} \in \mathcal{H}$ which implies $w^{T}x^{P} + b =0$
 therefore\\  $w^{T}x^{p} + b = w^{T}(x-d) + b = w^{T}(x-d) + b = w^{T}(x-\alpha w) + b =0$, which implies $\alpha = \frac{w^{T}x + b}{w^{T}w}$
 
 The length of $d$ is the euclidean norm \\$\vert \vert d \vert \vert_{2} = \sqrt{d^{T}d} = \sqrt{\alpha ^{2}w^{T}w} = \frac{\vert w^{T}x +b \vert}{\sqrt{w^{T}w}} = \frac{\vert w^{T}x + b \vert}{\vert \vert w \vert \vert_{2}}$
 
 Margin $\mathcal{H}$ with respect to $D:\gamma(w,b) = \min{x \in D} \frac{\vert w^{T}x + b \vert }  {\vert \vert w \vert \vert_{2}}$
 
 By definition, the margin and hyperplane are scale invariant : $\gamma(\beta w,\beta b) = \gamma (w,b) \forall \beta \neq 0$
 
 
 Note that if the hyperplane is such that $\gamma$ is maximized, it must lie right in the middle of the two classes. In other words, $\gamma$ must be the distance to the closest point within both classes. (If not, you could move the hyperplane towards data points of the class that is further away and increase $\gamma$, which contradicts that $\gamma$ is maximized.)
 
 
 
 
 Our aim is to find a hyperplane with maximum margin and  that all the points should be  correctly classified. This is a constrained optimization problem where the objective is the maximization of margin subject to the constraint that all the points must  be rightly classified.
 
 $\underbrace{\max_{w,b}  \ \gamma(w,b)}_\text{maximize margin}$ such that $\underbrace{ \forall i, \ y_{i}(w^{T}x_{i} + b) \geq 0}_\text{seperating hyperplane, such that all points are correctly classified}$
 
 
 By plugging in the definition of $\gamma$:
 
 $ \underbrace{max_{w,b} \  \frac{1} {\vert \vert w \vert \vert_{2} } \ \ min _{x_{i} \in D} \  \vert w^{T}x_{i} + b \vert }_\text{maximize margin}$ $s.t.$  $\underbrace{\forall i, \  y_{i}(w^{T}x_{i} + b) \geq 0}_\text{separating hyperplane}$ 
 
 
 Because the hyperplane is scale invariant, we can fix the scale of $w,b$ anyway we want. Let's  choose it such that 
 
 $$min_{x\in D}  \ \vert w^{T}x+b \vert = 1$$
 
 Now our objective becomes $max_{w,b} \  \frac{1} {\vert \vert w \vert \vert_{2}} . 1 = min_{w,b} \ \vert \vert w \vert \vert_{2} = min_{w,b} \  w^{T}w$
 
Thus the optimization problem becomes,
$$min_{w,b} \  w^{T}w$$
$$\forall i, \ y_{i}(w^{T}x_{i} + b) \geq 0$$
$$ min_{i} \  \vert w^{T}x_{i} +b \vert = 1$$ 

The two stated constraints for this optimization problem is equivalent.

$ min_{i} \  \vert w^{T}x_{i} +b \vert = 1$ is same as $ min_{i} \  y_{i}(w^{T}x_{i} + b) = 1$

Now $\forall i, \ y_{i}(w^{T}x_{i} + b) \geq 0$ and $min_{i} \ \ y_{i}(w^{T}x_{i} + b) = 1$, together this implies $ y_{i}(w^{T}x_{i} + b) \geq 1$
 
 
 
 
 
 
 $$min_{w,b} \  w^{T}w$$
 $$\forall i, \ y_{i}(w^{T}x_{i} + b) \geq 1$$
 
 
 
 
 
 
 
 
 
 
 
 
 Now, this is a convex optimization problem with quadratic objective and linear inequality constraints.
 
 
 For the optimal $w,b$ pair some training points will have tight constraints i.e $y_{i}(w^{T}x_{i} + b) = 1$.These training points are called \textbf{support vectors.} Support vectors  are the training points that define the maximum margin of the hyperplane to the data set and they therefore determine the shape of the hyperplane. 
 
 
 \subsection{Soft-Margin SVM}
 
 
 If the classes  overlap in the feature space i.e the linear inequality constraints are violated, there exist no solution to the optimization problem, this is usually the case with low dimensional data, but nevertheless we would like to have a hyperplane which can get most of the points correctly claassified with few violaion of constraints.
 
 
This is achieved by allowing the constraints to be violated ever so slight with the introduction of slack variables $\xi = (\xi_{1},\xi_{2}, ..., \xi_{N})$ 
   
   $$min_{w,b} \  w^{T}w + C \sum_{i =1}^{n} \xi_{i}$$
   $$ s.t. \ \forall i, \ y_{i}(w^{T}x_{i} + b) \geq 1- \xi_{i}$$
   $$ \forall i \ \xi_{i} \geq 0$$ 
   
The constant C decides how much slack we can use.The optimization problem  is a tradeoff between slack $ \xi$ and margin $\gamma$.We need to minimize $\vert \vert w \vert \vert$(for maximum margin) and $\sum \xi_{i}$(total slack) as we dont want too many constraints to be violated.
  
 The slack variable $\xi_{i}$ allows the input $x_{i}$ to be closer to the hyperplane (or even be on the wrong side), but there is a penalty in the objective function for such "slack". If C is very large, the SVM becomes very strict and tries to get all points to be on the right side of the hyperplane. If C is very small, the SVM becomes very loose and may "sacrifice" some points to obtain a simpler (i.e. lower$ \vert \vert w \vert \vert_{2} ^{2}$) solution. 
 
 The value $\xi_{i}$ is proportioal amount by which the prediction is on wrong side of the margin.Hence for points well inside the boundary the slack is zero and do not influence shape of the hyperplane.
 
 If $C =0 $ ,total slack is zero and that means slack is free and it's possible to violate as many as constraints, this implies $w = 0$.If $C = \infty$,slack is infinitely costly.So we donts use slack, we fall back to hard margin-SVM. 
 
 \section{Principal Component Analysis}
PCA is a dimensionality reduction algorithm, which aims to find those directions in which most of the variance of data lies i.e is it  identifies the subspace in which the data approximately lies.Inorder to run, PCA the data needs to be preprocessed to normalize it's mean and variance.The preprocessing procedure is as follows:

\begin{enumerate}
	\item Let $\mu  = \frac{1}{m} \sum_{i=1}^{m}x^{i}$
	\item Replace each $x^{(i)}$ with $x^{(i)}-\mu$ (zero out mean of the data)
	\item Let $\sigma _{j}^{2} = \frac{1}{m} \sum_{i}(x_{j}^{(i)})^{2}$
	\item  Replace each $x_{j}^{(i)}$ with $x_{j}^{(i)} /  \sigma_{j}$ \ (rescale each coordinate to have unit varaince,which ensures that different attributes are all treated on the same
	“scale.”)
\end{enumerate}

The dimensionality reduction is achieved by projecting the data into directions which capturre most of the variance of the data.

Let $x^{(i)} \in \mathbb{R}^{n}$ be a point in our dataset and $u$ be a unit vector. $x^{(i)}$'s projection onto $u$ is $x^{(i)^T}u$.Hence, to maximize the variance of the projections, we would like to choose a unit-length $u$ so as to maximize:
 
 $$\frac{1}{m} \sum_{i=1}^{m}(x^{(i)^T}u)^{2} = \frac{1}{m} \sum_{i=1}^{m} u^{T}x^{(i)}x^{(i)^T}u = u^{T} \left(\frac{1}{m} \sum_{i=1}^{m} x^{(i)}x^{(i)^T} \right)u$$
 
 Maximizing this subject to  $\norm{u}_{2}=1$ gives the principal eigenvector of $\Sigma = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}x^{(i)^T} ,$ which is the emperical covariance matrix of the data,assuming it has zero mean. \href{https://math.stackexchange.com/questions/1199852/maximize-the-value-of-vtav}{read}
 
 The direction of maximum variance turns out to be eigenvectors of the covariance matrix.If we wish to project our data into a k-dimensional subspace $(k < n)$, we should choose $u_{1}, ..., u_{k}$ to be the top $k$ eigenvectors of $\Sigma$. The $u_{i}$s now form a new, orthogonal basis for the data.
 
 A datapoint $x^{(i)}$ projected into a low(say $k<n$)-dimensional subspace  spanned by top $k$ eigenvectors of the covariance matrix is given by:
 $$y^{(i)} =\begin{bmatrix} 
 	& u_{1}^{T}x^{(i)} &  \\
 	& u_{2}^{T}x^{(i)} &\\
 	& \vdots & \\
 	& u_{k}^{T}x^{(i)} &
 \end{bmatrix}  \in \mathbb{R}^{k}$$
 The vectors  $u_{1}, ..., u_{k}$ are called the first principal components of the data.
 
  $$ Y =\begin{bmatrix} 
 \	& y^{(1)} & \ \\
 \	& y^{(2)} & \ \\
 \	& \vdots &  \ \\
 \	& y^{(m)} &\
 \end{bmatrix}  \in \mathbb{R}^{m \times k}$$
 
 
 
 $$ X =\begin{bmatrix} 
 & x^{(1)} &  \\
 & x^{(2)} &\\
 & \vdots & \\
 & x^{(m)} &
 \end{bmatrix}  \in \mathbb{R}^{m \times n}$$
 
$$ U =\begin{bmatrix} 
\vert & \vert&  &\vert \\
u_{1} & u_{2} & \hdots & u_{k}\\
 
 \vert & \vert&  &\vert
 \end{bmatrix}  \in \mathbb{R}^{n \times k}$$ U is an orthogonal matrix, each column vector is an eigenvector of a  covariance matrix.
 
 $$\underbrace{Y}_\text{low-dimesional  data}= \underbrace{X} _\text{original data}  \times \underbrace{U}_\text{k-principal components}$$
 
$$ \begin{bmatrix} 
\	& y^{(1)} & \ \\
\	& y^{(2)} & \ \\
\	& \vdots &  \ \\
\	& y^{(m)} &\
\end{bmatrix}   =
 \begin{bmatrix} 
	& x^{(1)} &  \\
	& x^{(2)} &\\
	& \vdots & \\
	& x^{(m)} &
\end{bmatrix}  \times 
 \begin{bmatrix} 
	\vert & \vert&  &\vert \\
	u_{1} & u_{2} & \hdots & u_{k}\\
	
	\vert & \vert&  &\vert
\end{bmatrix}  $$


We can reconstrut the data by projecting back into the original canonical basis. $Y = XU$, $YU^{T} = XUU^{T} \implies YU^{T} = X$.
PCA can also be thought as an algorithm to choose the basis that minimizes the approximation(reconstruction also) error arising from projecting the
data onto the k-dimensional subspace spanned by them.

 
 
 
 
 
 
 
 
 
  
 
\end{document}