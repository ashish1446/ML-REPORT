\documentclass[12pt]{article}
%\renewcommand{\thesection}{}
\usepackage{mathtools}
\usepackage{commath}
\usepackage[sc,osf]{mathpazo}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
%\usepackage[plain]{algorithm}
%\usepackage{algorithmic}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

%\usepackage[noend]{algorithmic} 
\usepackage{algorithmic} 
\usepackage{algorithm,caption}
\algsetup{indent=2em} 
\renewcommand{\algorithmiccomment}[1]{\hspace{2em}// #1} 

\newtheorem*{thm}{Theorem}



\usepackage{hyperref}
\hypersetup{
	colorlinks=false,
	linkcolor=red,
	filecolor=red,      
	urlcolor=red,
}

\urlstyle{same}



\begin{document}
	
	   
	     
	
		\enlargethispage{2cm}
		
		\begin{center}
			
			\vspace*{-1cm}
			
			\textbf{\Large Notes on Machine Learning     }(incomplete)\\[10pt]
			

\textbf{\Large Aswin}\\ [8pt]			
			
			\end{center}
		
\cleardoublepage

\tableofcontents
\newpage

\section{Overview}

I made this document as a way to learn ML for my Maters' thesis . It’s not an original work,but a compilation of scribed notes of the ML course by Dr Sanjoy Dasgupta(UCSD), [SD] which I audited.Some parts are drawn directly/inspired from Andrew NG's Stanford CS229 course notes, [ANG] and Kilian weinberger's Cornell CS4780 course notes[KW].My primary reference was 'The Elements of Statistical Learning' by Trevor Hastie,Robert Tibshirani,Jerome Friedman, [HTF].So there will be considerable overlap with the aforementioned materials.This note might lack mathematical rigor but I have tried to give proofs and supplementary topics wherever necessary.For each algorithm, I have given a url to Github repo containing the implemetation using one of the three datasets viz Fisher's Iris dataset, Wisconsin Breast Cancer Dataset and MNIST handwritten digits dataset.
Please write to me if you find any inaccuracies.I hope this   proves at least moderately
interesting or useful for you.

\cleardoublepage

\section{Supervised Learning}

	In a typical scenario, we have an outcome measurement, usually quantitative (such as a stock price) or
categorical (such as heart attack/no heart attack), that we wish to predict
based on a set of features (such as diet and clinical measurements). We
have a training set of data, in which we observe the outcome and feature
measurements for a set of objects (such as people). Using this data we build
a prediction model, or learner, which will enable us to predict the outcome
for new unseen objects. A good learner is one that accurately predicts such
an outcome.
The examples above describe what is called the supervised learning problem. It is called “supervised” because of the presence of the outcome variable to guide the learning process.[HTF]

 


\subsection{Variable Types and Terminology}

The outcome measurement which we wish to predict denoted as \textbf{\textit{outputs}} depend on a set of variebles denoted as \textbf{\textit{inputs}}.
Classicaly,the \textit{inputs} are independent varibales whereas \textit{outputs} are dependent variables.
The term \textit{features} will be used interchangeably with inputs.

The \textit{outputs} which we wish to predict can be qualitative or quantitative(as in blood sugar level).
When the \textit{outputs} are qualitative (as in spams or  not spams),it is referred as categorical or discrete variables and are typically represented numerically by codes,as in -spam or not spam can be coded as -1 or 1.
Depending upon the kind of output varible, the prediction task can be of two types: \textit{regression} when we predict quantitative outputs and \textit{classification} when we predict  qualitative outputs.




 The input variables/features are denoted by $x^{(i)}$ and  the space of all such $x^{(i)}$ is $X$.
 The output variable that we are trying to predict is denoted as $y^{(i)}$ and the space of all such $y^{(i)}$ is $Y$.
 A pair $(x^{(i)},y^{(i)})$ is called a training example and the dataset, we will be using to learn - a collection of $n$ training examples $ \{(x^{(i)},y^{(i)});i=1,2,...,n\} $ - is called a training set.
 The superscript $“(i)”$ in the notation is simply an index into the training set.
 
 


\cleardoublepage

\section{Linear Regression}

Given a vector of inputs, $x^{T}=(x_{1},x_{2},..., x_{d})$.We need to know functions/hypotheses $h$ that can  approximate $y$ as a linear function of $x$: $$ h_{\theta}(x)=\theta_{0} + \sum_{i=1}^{d}x_{j}\theta_{j}$$
$\theta_{i}$s are parameters/weight parametrizing the space of linear functions mapping from $X$ to $Y$.The term $\theta_{0}$ is the intercept, also known as the \textit{bias} in machine learning.It is covenient to include $\theta_{0}$ in the vector of weights $\theta$  and add constant variable $1$ to the vector $x$, so that $$ h_{\theta}(x) = \theta^{T}x$$ $w$ ( weights) and $b$  (bias) can be interchangably used with $\theta$ and $\theta_{0}$ respectively.




For a training set,we have to  learn the parameters $\theta,$ so that we can predict $y$. One reasonable method seems to be to make $h(x)$ close to y,for the training examples.To formalize this, we will define a function that measures, for each value of the $\theta$'s, how close the $h(x^{(i)})'$ s are to the corrsponding $y^{(i)}$'s.

We  define Cost/Loss function: $$J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^{2},$$this is the Least Squares cost function.In this approach, we pick the coefficients $\theta$ to minimize the cost function $J$.The LS cost function is quadratic function in weights, $\theta$ and hence it's minimum always exist, but may not be unique.
Given a set of training examples $(x^{(i)},y^{(i)})$ i.e training set ,define a matrix $X$ to be the $m$-by-$n$ matrix  that contains the input values of training examples in it's rows:  

 $$ \begin{bmatrix} 
  & {(x^{(1)})^{T}} &  \\
  & \vdots & \\
  &   (x^{(m)})^{T}     &  
  \end{bmatrix} $$
 
 
Let $ \textbf y$ be the $m$-dimensional vector containing the target/output values from the training set:
 $$\begin{bmatrix} 
 & y^{(1)} &  \\
 & \vdots & \\
 &   y^{(m)}     &  
 \end{bmatrix}$$
 
 
Since $h_{\theta}(x^{(i)})=(x^{(i)})^{T}\theta$,we can verify that
 $$X \theta - \textbf y = \begin{bmatrix} 
 & {(x^{(1)})^{T}}\theta &  \\
 & \vdots & \\
 &   (x^{(m)})^{T} \theta    &  
 \end{bmatrix} - \begin{bmatrix} 
 & y^{(1)} &  \\
 & \vdots & \\
 &   y^{(m)}     &  
 \end{bmatrix}$$
 
 $$=\begin{bmatrix} 
 & h_{\theta}(x^{(1)})-y^{(1)} &  \\
 & \vdots & \\
 &  h_{\theta}(x^{(m)})- y^{(m)}     &  
 \end{bmatrix}$$
 
 
 $\frac{1}{2}(X\theta-\textbf y)^{T}(X\theta-\textbf y)=\frac{1}{2} \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}=J(\theta)$ This is the cost function.To minimize $J$,we have to find the derivatives with respect to $\theta$
 
 
 
 
Some matrix derivative results
 $$\nabla_{A}tr AB = B^{T}$$ 
 $$\nabla_{A}\vert A\vert=\vert A\vert(A^{-1})^{T}$$ 
 $$\nabla_{A^{T}}f(A)= (\nabla_{A}f(A))^{T}$$ 
 $$\nabla_{A}trABA^{T}C=CAB+C^{T}AB^{T}$$
 
 
 Using the last two results 
 $$\nabla_{A^{T}}trABA^{T}C=B^{T}A^{T}C^{T}+BA^{T}C$$
 
 
 
 The cost function,  $J(\theta)=\frac{1}{2}(X\theta-\textbf{y})^{T}(X\theta-\textbf{y})$
 
 
$$ \nabla_{\theta} J(\theta)  =\nabla_{\theta}\frac{1}{2}(X\theta-\textbf{y})^{T}(X\theta-\textbf{y}) $$
 
 
 
  $$=\frac{1}{2}\nabla_{\theta}(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}\textbf{y}-\textbf{y}^{T}X\theta+\textbf{y}^{T}\textbf{y}) $$
 
 
 
$$ =\frac{1}{2}\nabla_{\theta} tr(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}\textbf{y}-\textbf{y}^{T}X\theta+\textbf{y}^{T}\textbf{y}) $$
 
 
 
 
$$=\frac{1}{2}\nabla_{\theta}(tr \theta^{T}X^{T}X\theta-2tr\textbf{y}^{T}X\theta)  $$
 
 

$$=\frac{1}{2}(X^{T}X\theta+X^{T}X\theta-2X^{T}\textbf{y})  =X^{T}X\theta-X^{T}\textbf{y} $$







To minimize $J,$ we set the derivative to zero and obtain:$X^{T}(X\theta-\textbf{y})=0$. If $X^{T}X$ is nonsingular then the value of $\theta $ that minimizes $J(\theta)$ is given in closed form by the equation,
$$\theta=(X^{T}X)^{-1}X^{T}\textbf{y}$$
Now that we have the parameters,we can predict the output corresponding to the input $x^{(i)}$ as $y^{(i)}=\theta^{T}x^{(i)}$

\section{Logistic Regression}
Logistic regression is a proabilistic method,where a linear function of $x$, $wx + b $ is mapped to $[0,1]$ using our new hypothesis $h_{w}(x)$ defined as 

$$h_{w}(x) = g(w^{T}x) = \frac{1}{1 + \exp{(-w^{T}x)}} \ ,$$

Where $$g(z)= \frac{1}{1+ \exp{(-z)}}$$

is called the logistic function or the sigmoid function.
The learning problem is, given a data set and logistic regression model how will we find the parameter $w$.We can achieve this using probabilistic assumptions, and then fit the parameters via maximum likelihood.

Let us assume that 
$$P(y =1 \vert x;w) = h_{w}(x)$$
$$P(y =0 \vert  x;w) = 1 - h_{w}(x)$$

This together can be expressed as
 $$p(y \vert x;w) = (h_{w}(x))^{y}(1-h_{w}(x))^{1-y}$$
 
 Assuming that we have m training  examples were generated independently, we
 can then write down the likelihood of the parameters as
 
\begin{equation}
 \begin{split}
L(w) & = P(\textbf{y} \vert \textbf{X} ; w) \\
& = \prod_{i=1}^{m} p(y^{(i)} \vert x ^{(i)};w) \\
& = \prod_{i=1}^{m}  (h_{w}(x^{(i)}))^{y^{(i)}}(1-h_{w}(x^{(i)}))^{1-y^{(i)}} 
 \end{split}
\end{equation}


Maximizing the likelhood is equivalent to maximizing any strictly incresing function of likelihood.

\begin{equation}
\begin{split}
l(w) & = log L(w) \\ 
& = \sum_{i =1}^{m} y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log(1-h(x^{(i)}))
\end{split}
\end{equation}

We can use gradient descent to maximize the above equation and the update rule will be $w := w +\alpha \nabla _{w}l(w)$.For a single training example,




$$\frac{\partial l(w)}{\partial w_{j}} = (y - h_w(x))x_{j}   $$

So the update rule for stochastic gradient ascent becomes $$w_{j} := w_{j} +\alpha (y^{(i)} - h_w(x^{(i)}))x^{(i)}_{j}$$


Find an implementation \href{https://github.com/aswin16/ML-REPORT/blob/master/codes/log_regression.ipynb}{here}


\section{The Perceptron}
In a binary classification problem , where dataset(D) is  $(x,y) \in \mathbb{R}^{d} \times\{-1,1\}$, the  learning problem is to find a hyperplane which separates the data into two classes, 
 assuming the data is linearly classifiable. The hyperplane is parametrized by $w\in\mathbb{R}^{d}$ and $ b \in \mathbb{R}$ such that $w.x + b = 0 $, so the poblem is equivalent to learning  the parameters $w$ and $b$.On point $x$, we predict the label as $\textbf{sign(w.x + b)}$.\\
$$(w.x + b) > 0  \implies y = +1 \therefore y(w.x + b) > 0 $$

$$(w.x + b) < 0  \implies y = -1 \therefore y(w.x + b) > 0 $$
 
i.e., If the true label of x is y, then $y(w.x + b) > 0 $, whereas for a misclassified pointi$y(w.x + b) \leq 0 $. The Loss function  for the perceptron can be defined as 





\begin{equation} 
Loss=
\begin{cases}
0, & \text{if} \ y(w.x + b) > 0 
 \\
-y(w.x + b) , & \text{if} \ y(w.x + b) \leq 0 
\end{cases}
\end{equation}

This loss function is a convex function of $y(w.x + b)$, and we can use stochastic gradient descent to find the value of  parameters that minimizes the loss function.
The update on the parameter w can be written as $w := w - \eta \nabla L(w)$,where $w = [w,b]$.The derivative of Loss function with respect to the parameters(assuming bias term is not absorbed into the weight vector) are 

$$\frac{\partial L}{\partial w} = -yx , \frac{\partial L}{\partial b} = -y$$.
 \cleardoublepage
So the update rule will be $w := w + \eta yx $ and $b := b + \eta y$. For $w = [w,b]$, this is equivalent  to  $w := w + \eta yx $.
If $\eta = 1$, then the update rule will be  $w := w +  yx $.

\begin{algorithm}
	
	\caption*{Perceptron Algorithm} \label{alg:MyAlgorithm}
	\begin{algorithmic}
		\STATE Initialize $\vec{w} = 0$
		
		\WHILE {TRUE }
		\STATE $m=0$
		\FOR{$(x_{i},y_{i}) \in D$}
		
		\IF {$y_{i}(\vec{w}^{T} . x_{i})\leq 0$}
		
		\STATE $\vec{w} \leftarrow \vec{w} + yx$
		\STATE $m \leftarrow m + 1$
		
		\ENDIF
		\ENDFOR
		
		\IF {$m = 0$}
		\STATE break
		
		\ENDIF
		
		
		
		\ENDWHILE
		
	\end{algorithmic}
\end{algorithm}




 If the taining data is linearly classifiable, Perceptron is guaranteed to converge after finite number of steps and return a seperating hyperplane with zero training error.
 

 
 \textbf{Margin $\gamma$ of a hyperplane} $w$ is defined as $\gamma = min_{(x_{i},y_{i})\in D}  \frac{ \vert x_{i}^{T}w \vert}{\|w\|_{2}}$, i.e it is the distance to the closest data point from the hyperplane paramtrized by $w$.
 
 
 
 \begin{thm}If a data set is linearly separable, the Perceptron will find a separating hyperplane in a finite number of updates.\end{thm}
 
 \begin{proof}
 	Suppose $\exists w^{*}$ such that $ y_{i}(x^{T}w^{*}) > 0$ $ \forall (x_{i},y_{i}) \in D$.Suppose that we rescale each data point and the $w^{*}$ such that
 	
 	
 	$$\|w^{*}\| = 1 \ and \ \|x_{i}\| \leq 1 \forall x_{i} \in D $$
 	
    So the margin $\gamma$ for the hyperplane $w^{*}$ becomes $ \gamma = min_{(x_{i},y_{i})\in D} \vert x_{i}^{T}w^{*} \vert $. After rescaling, all inputs   $x_{i}$ lies in a unit sphere in d-dimensional space.The separating hyperplane is defined  by $w^{*}$ with $\|w\|^{*} = 1$ i.e is \textbf{$w^{*}$} lies exactly on the unit sphere.
    
    We claim that if the above assumptions hold,then the Perceptron algorithm makes atmost $\frac{1}{\gamma ^{2}}$ mistakes.The update on w is only done in the instance of misclassification, i.e when  $y(x^{T}w) \leq 0$ holds.As $w^{*}$ is a seperating hyper-plane and classifies all points correctly,  $y(x^{T}w)>0$ $ \forall x$.
    
     Consider the effect of an update $w \leftarrow w + xy $ on the two terms $w^{T}w^{*}$ and $w^{T}w$.
     
     $w^{T}w^{*} = (w+xy)^{T}w^{*} = w^{T}w^{*} + y(x^{T}w^{*}) \geq w^{T}w^{*} + \gamma $
        
    The inequality follows from the fact that, for $w^{*}$, the distance from the hyperplane defined by $w^{*}$ to $x$ must be at least $\gamma$ i.e
    $y(x^{T}w^{*}) = \vert x^{T}w^{*}\vert \geq \gamma$.This implies that with each update $w^{T}w^{*}$ grows atleast by $\gamma$
 
     $w^{T}w = (w+xy)^{T}(w+xy) = w^{T}w + \underbrace{2y(w^{T}x)}_\text{$ \leq 0$} + \underbrace{y^{2}(x^{T}x)}_\text{$0 \leq \ \leq 1$}  \leq
     w^{T}w + 1$.
     This inequality follows the fact that, $2y(w^{T}x) \leq 0$,  as we had to make an update, meaning $x$ was misclassified. $0 \leq y^{2}(x^{T}x) \leq 1$ as $y^{2}=1 $ always and all $x^{T}x \leq 1$ as $\|x\| \leq 1$,(rescaled). This implies that $w^{T}w$ grows at most by 1.
     
     
   %  \cleardoublepage
     
     After $M$ updates, the two inequalities becomes $$w^{T}w^{*} \geq M\gamma$$  $$w^{T}w \leq M$$
 
 $$M\gamma \leq w^{T}w^{*} \leq \vert w^{T}w^{*} \vert \leq \| w^{T} \| \underbrace{\|w^{*}\|}_\text{1}=\sqrt{w^{T}w}$$
 
 $w^{T}w$ can most be $M$, as $w$ is initialized with 0 and with each update $w^{T}w$ grows at most by 1.
 
 $$\implies M\gamma \leq \sqrt{M} $$
 $$\implies M^{2}\gamma^{2} \leq M$$
 $$\implies M \leq \frac{1}{\gamma ^{2}}$$  Hence the number of updates $M$ is bounded from above by a constant.
  
 
 %	\underbrace{(x + 2)^3}_\text{text 1}
 	
 \end{proof}
 
 \section{Boosting}
 \textbf A {Weak Classifier} is the one with accuracy marginally better than random guessing. For a binary  weak classifier this means, $P(h(x) \neq y) = \frac{1}{2} - \epsilon .$ An learning algorithm which consistently generate such weak classifier is called a \textbf{weak learner}.\\ \textbf{Boosting} is a machine learning approach where such weak learners are combined to get better prediction acuracy.\\


%\cleardoublepage

 \subsection{AdaBoost}
 
 
 \begin{algorithm}
 	
 	\caption*{AdaBoost Algorithm} \label{alg:MyAlgorithm}
 	\begin{algorithmic}
 		
 		\STATE 1. \ Given $(x^{(i)},y^{(i)}),...,(x^{(N)},y^{(N)}),$ where $y^{(i)}\in \{-1,+1\}$ 
 		\STATE 2. \  Initialize the observation weights $w_{i}= \frac{1}{N},i = 1,2,...,N.$
 		
 		\STATE 3. \ For m =1 to M:
 		
 		
 		
 		\STATE \hspace{ 0.8cm}(a) \ Fit a classifier $h_{m}(x)$ to the trainng data using weights $w_{i}.$
 		
 		\STATE \hspace{ 0.8cm}(b) Compute 
 		
 	
 		
 			$$err_{m} = \frac{\sum_{i=1}^{N}w_{i}y^{(i)}h_{m}(x^{(i)})}{\sum_{i=1}^{N} w_{i}}$$
 		
 		
 		
 			\STATE \hspace{ 0.8cm}(c) compute $\alpha_{m} = \log((1-err_{m})/err_{m})$
 			
 				\STATE \hspace{ 0.8cm}(d) set $w_{i} \leftarrow w_{i}.exp[-\alpha_{m}.y^{(i)}h_{m}(x^{(i)})], \ i = 1,2,...,N$
 				
 	\STATE 4. \ Output $H(x) = sign[\sum_{m=1}^{M}\alpha_{m}G_{m}(x)]$
 		
 		
 	\end{algorithmic}
 \end{algorithm}
 
 
 
 
 
 \section{Nearest Neighbour Methods}
 
 Nearest-neighbour methods use those observations in the taining set $\mathcal{T}$ closest in input space to $x$ to form $\hat{Y}$.
 The k-nearest neighbour fit for $\hat{Y}$ is defined as follows:
 
 $$\hat{Y(x)} = \frac{1}{k}\sum_{x_{i} \in N_{k}(x)} y_{i},$$ where $N_{k}$ is neighbourhood of $x$ defined by $k$ closest points $x_{i}$ in the training sample.
 
 Closeness is quantified by a metric, which for instance can be assumed as Euclidean distance.We find $k$ observations with $x_{i}$ closes to x in the input space of training set, and average their responses.
 
 The notion of distance between two vectors is defined by a norm. A norm is a function from a vector space over the real or complex numbers to the nonnegative real numbers that satisfies certain properties pertaining to scalability and additivity, and takes the value zero if only the input vector is zero.
 
 As mentioned above we will use the Euclidean norm for all practical purposes.The Euclidean norm  is a specific norm on a vector space, that is strongly related with the Euclidean distance, and equals the square root of the inner product of a vector with itself.On an n-dimensional Euclidean space $ R^{n}$, the intuitive notion of length of the vector $x = (x1, x2, ..., xn)$ is captured by the formula
 
 $$\norm{x}_{2}= \sqrt{x_{1}^{2}+ \dots x_{n}^{2}}$$
 
 
This definition of norm gives distance between two vectors as the euclidean norm of the component wise difference i.e if $x, y \in R^{n}$ then \\ $ \norm{(x-y)}_{2} = \sqrt{\sum\limits_{i=1}^{n}(x_{i}-y_{i})^{2}} $
 
 \vspace{0.4 cm}
 Find an implementation of NN method \href{https://github.com/aswin16/ML-REPORT/blob/master/codes/NN_MNIST.ipynb}{here}
 
 
 
 
 
 \section{Support Vector Machines \tiny{[HTF][KW]}} 
 
 The Perceptron algorithm assures to return a linear seperating hyperplane if one exists.Existence of one  such  hyperplane implies that there is infinite such hyperplanes, and the perceptron doesn't guarantee to return the optimal seperating hyperplane i.e, the one with maximum margin.
 Support vector machines(SVM) finds maximum margin hyperplane. 
 
 The training dataset is $(x^{(i)}, y^{(i)}) \in \mathbb{R}^{d} \{-1,+1\},i = 1,...,N$ We define a  hyperplane by $\mathcal{H}=\{x \vert w^{T}x + b =0\}$ paramtrized by w and b.Let the margin $\gamma$ be defined as the distance from the hyperplane to the closest point across both the classes. A classifiction rule induced by the hyperplane is $h(x) = sign(w^{T}x+ b)$
 
 
 Consider some point $x$. Let $d$ be the vector from $\mathcal{H}$ to $x$ of minimum length. Let $x^{P}$ be the projection of $x$ onto $\mathcal{H}$. It follows then that:
 
 $x^{P} = x - d.$
 $w$ is perpendicular to $\mathcal{H}$. $d$ is parallel to $w$,  so $d = \alpha w$ for some $\alpha \in \mathbb{R}$. $x^{P} \in \mathcal{H}$ which implies $w^{T}x^{P} + b =0$
 therefore\\  $w^{T}x^{p} + b = w^{T}(x-d) + b = w^{T}(x-d) + b = w^{T}(x-\alpha w) + b =0$, which implies $\alpha = \frac{w^{T}x + b}{w^{T}w}$
 
 The length of $d$ is the euclidean norm \\$\vert \vert d \vert \vert_{2} = \sqrt{d^{T}d} = \sqrt{\alpha ^{2}w^{T}w} = \frac{\vert w^{T}x +b \vert}{\sqrt{w^{T}w}} = \frac{\vert w^{T}x + b \vert}{\vert \vert w \vert \vert_{2}}$
 
 Margin $\mathcal{H}$ with respect to $D:\gamma(w,b) = \min{x \in D} \frac{\vert w^{T}x + b \vert }  {\vert \vert w \vert \vert_{2}}$
 
 By definition, the margin and hyperplane are scale invariant : $\gamma(\beta w,\beta b) = \gamma (w,b) \forall \beta \neq 0$
 
 
 Note that if the hyperplane is such that $\gamma$ is maximized, it must lie right in the middle of the two classes. In other words, $\gamma$ must be the distance to the closest point within both classes. (If not, you could move the hyperplane towards data points of the class that is further away and increase $\gamma$, which contradicts that $\gamma$ is maximized.)
 
 
 
 
 Our aim is to find a hyperplane with maximum margin and  that all the points should be  correctly classified. This is a constrained optimization problem where the objective is the maximization of margin subject to the constraint that all the points must  be rightly classified.
 
 $\underbrace{\max_{w,b}  \ \gamma(w,b)}_\text{maximize margin}$ such that $\underbrace{ \forall i, \ y_{i}(w^{T}x_{i} + b) \geq 0}_\text{seperating hyperplane, such that all points are correctly classified}$
 
 
 By plugging in the definition of $\gamma$:
 
 $ \underbrace{max_{w,b} \  \frac{1} {\vert \vert w \vert \vert_{2} } \ \ min _{x_{i} \in D} \  \vert w^{T}x_{i} + b \vert }_\text{maximize margin}$ $s.t.$  $\underbrace{\forall i, \  y_{i}(w^{T}x_{i} + b) \geq 0}_\text{separating hyperplane}$ 
 
 
 Because the hyperplane is scale invariant, we can fix the scale of $w,b$ anyway we want. Let's  choose it such that 
 
 $$min_{x\in D}  \ \vert w^{T}x+b \vert = 1$$
 
 Now our objective becomes $max_{w,b} \  \frac{1} {\vert \vert w \vert \vert_{2}} . 1 = min_{w,b} \ \vert \vert w \vert \vert_{2} = min_{w,b} \  w^{T}w$
 
Thus the optimization problem becomes,
$$min_{w,b} \  w^{T}w$$
$$\forall i, \ y_{i}(w^{T}x_{i} + b) \geq 0$$
$$ min_{i} \  \vert w^{T}x_{i} +b \vert = 1$$ 

The two stated constraints for this optimization problem is equivalent.

$ min_{i} \  \vert w^{T}x_{i} +b \vert = 1$ is same as $ min_{i} \  y_{i}(w^{T}x_{i} + b) = 1$

Now $\forall i, \ y_{i}(w^{T}x_{i} + b) \geq 0$ and $min_{i} \ \ y_{i}(w^{T}x_{i} + b) = 1$, together this implies $ y_{i}(w^{T}x_{i} + b) \geq 1$
 
 
 
 
 
 
 $$min_{w,b} \  w^{T}w$$
 $$\forall i, \ y_{i}(w^{T}x_{i} + b) \geq 1$$
 
 
 
 
 
 
 
 
 
 
 
 
 Now, this is a convex optimization problem with quadratic objective and linear inequality constraints.
 
 
 For the optimal $w,b$ pair some training points will have tight constraints i.e $y_{i}(w^{T}x_{i} + b) = 1$.These training points are called \textbf{support vectors.} Support vectors  are the training points that define the maximum margin of the hyperplane to the data set and they therefore determine the shape of the hyperplane. 
 
 
 \subsection{Soft-Margin SVM}
 
 
 If the classes  overlap in the feature space i.e the linear inequality constraints are violated, there exist no solution to the optimization problem, this is usually the case with low dimensional data, but nevertheless we would like to have a hyperplane which can get most of the points correctly claassified with few violaion of constraints.
 
 
This is achieved by allowing the constraints to be violated ever so slight with the introduction of slack variables $\xi = (\xi_{1},\xi_{2}, ..., \xi_{N})$ 
   
   $$min_{w,b} \  w^{T}w + C \sum_{i =1}^{n} \xi_{i}$$
   $$ s.t. \ \forall i, \ y_{i}(w^{T}x_{i} + b) \geq 1- \xi_{i}$$
   $$ \forall i \ \xi_{i} \geq 0$$ 
   
The constant C decides how much slack we can use.The optimization problem  is a tradeoff between slack $ \xi$ and margin $\gamma$.We need to minimize $\vert \vert w \vert \vert$(for maximum margin) and $\sum \xi_{i}$(total slack) as we dont want too many constraints to be violated.
  
 The slack variable $\xi_{i}$ allows the input $x_{i}$ to be closer to the hyperplane (or even be on the wrong side), but there is a penalty in the objective function for such "slack". If C is very large, the SVM becomes very strict and tries to get all points to be on the right side of the hyperplane. If C is very small, the SVM becomes very loose and may "sacrifice" some points to obtain a simpler (i.e. lower$ \vert \vert w \vert \vert_{2} ^{2}$) solution. 
 
 The value $\xi_{i}$ is proportional amount by which the prediction is on wrong side of the margin.Hence for points well inside the boundary the slack is zero and do not influence shape of the hyperplane.
 
 If $C =0 $ ,total slack is zero and that means slack is free and it's possible to violate as many as constraints, this implies $w = 0$.If $C = \infty$,slack is infinitely costly.So we donts use slack, we fall back to hard margin-SVM. 
 
 \section{Principal Component Analysis}
PCA is a dimensionality reduction algorithm, which aims to find those directions in which most of the variance of data lies i.e is it  identifies the subspace in which the data approximately lies.Inorder to run, PCA the data needs to be preprocessed to normalize it's mean and variance.The preprocessing procedure is as follows:

\begin{enumerate}
	\item Let $\mu  = \frac{1}{m} \sum_{i=1}^{m}x^{i}$
	\item Replace each $x^{(i)}$ with $x^{(i)}-\mu$ (zero out mean of the data)
	\item Let $\sigma _{j}^{2} = \frac{1}{m} \sum_{i}(x_{j}^{(i)})^{2}$
	\item  Replace each $x_{j}^{(i)}$ with $x_{j}^{(i)} /  \sigma_{j}$ \ (rescale each coordinate to have unit varaince,which ensures that different attributes are all treated on the same
	“scale.”)
\end{enumerate}

The dimensionality reduction is achieved by projecting the data into directions which capturre most of the variance of the data.

Let $x^{(i)} \in \mathbb{R}^{n}$ be a point in our dataset and $u$ be a unit vector. $x^{(i)}$'s projection onto $u$ is $x^{(i)^T}u$.Hence, to maximize the variance of the projections, we would like to choose a unit-length $u$ so as to maximize:
 
 $$\frac{1}{m} \sum_{i=1}^{m}(x^{(i)^T}u)^{2} = \frac{1}{m} \sum_{i=1}^{m} u^{T}x^{(i)}x^{(i)^T}u = u^{T} \left(\frac{1}{m} \sum_{i=1}^{m} x^{(i)}x^{(i)^T} \right)u$$
 
 Maximizing this subject to  $\norm{u}_{2}=1$ gives the principal eigenvector of $\Sigma = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}x^{(i)^T} ,$ which is the emperical covariance matrix of the data,assuming it has zero mean. \href{https://math.stackexchange.com/questions/1199852/maximize-the-value-of-vtav}{$v^{T}Tv$ is maximized if $v$ is eigenvector of $T$}
 
 The direction of maximum variance turns out to be eigenvectors of the covariance matrix.If we wish to project our data into a k-dimensional subspace $(k < n)$, we should choose $u_{1}, ..., u_{k}$ to be the top $k$ eigenvectors of $\Sigma$. The $u_{i}$s now form a new, orthogonal basis for the data.
 
 A datapoint $x^{(i)}$ projected into a low(say $k<n$)-dimensional subspace  spanned by top $k$ eigenvectors of the covariance matrix is given by:
 $$y^{(i)} =\begin{bmatrix} 
 	& u_{1}^{T}x^{(i)} &  \\
 	& u_{2}^{T}x^{(i)} &\\
 	& \vdots & \\
 	& u_{k}^{T}x^{(i)} &
 \end{bmatrix}  \in \mathbb{R}^{k}$$
 The vectors  $u_{1}, ..., u_{k}$ are called the first principal components of the data.
 
  $$ Y =\begin{bmatrix} 
 \	& y^{(1)} & \ \\
 \	& y^{(2)} & \ \\
 \	& \vdots &  \ \\
 \	& y^{(m)} &\
 \end{bmatrix}  \in \mathbb{R}^{m \times k}$$
 
 
 
 $$ X =\begin{bmatrix} 
 & x^{(1)} &  \\
 & x^{(2)} &\\
 & \vdots & \\
 & x^{(m)} &
 \end{bmatrix}  \in \mathbb{R}^{m \times n}$$
 
$$ U =\begin{bmatrix} 
\vert & \vert&  &\vert \\
u_{1} & u_{2} & \hdots & u_{k}\\
 
 \vert & \vert&  &\vert
 \end{bmatrix}  \in \mathbb{R}^{n \times k}$$ U is an orthogonal matrix, each column vector is an eigenvector of a  covariance matrix.
 
 $$\underbrace{Y}_\text{low-dimesional  data}= \underbrace{X} _\text{original data}  \times \underbrace{U}_\text{k-principal components}$$
 
$$ \begin{bmatrix} 
\	& y^{(1)} & \ \\
\	& y^{(2)} & \ \\
\	& \vdots &  \ \\
\	& y^{(m)} &\
\end{bmatrix}   =
 \begin{bmatrix} 
	& x^{(1)} &  \\
	& x^{(2)} &\\
	& \vdots & \\
	& x^{(m)} &
\end{bmatrix}  \times 
 \begin{bmatrix} 
	\vert & \vert&  &\vert \\
	u_{1} & u_{2} & \hdots & u_{k}\\
	
	\vert & \vert&  &\vert
\end{bmatrix}  $$


We can reconstrut the data by projecting back into the original canonical basis. $Y = XU$, $YU^{T} = XUU^{T} \implies YU^{T} = X$.
PCA can also be thought as an algorithm to choose the basis that minimizes the approximation(reconstruction also) error arising from projecting the
data onto the k-dimensional subspace spanned by them.

 
 Find an implementation of PCA with MNIST data \href{https://github.com/aswin16/ML-REPORT/blob/master/codes/PCA.ipynb}{here}.
 
 \section{Decision Trees}
 
  Most data that is interesting has some inherent structure. In the k-NN case we make the assumption that similar inputs have similar neighbors.. This would imply that data points of various classes are not randomly sprinkled across the space, but instead appear in clusters of more or less homogeneous class assignments. Although there are efficient data structures enable faster nearest neighbor search, it is important to remember that the ultimate goal of the classifier is simply to give an accurate prediction. Imagine a binary classification problem with positive and negative class labels. If you knew that a test point falls into a cluster of 1 million points with all positive label, you would know that its neighbors will be positive even before you compute the distances to each one of these million distances. It is therefore sufficient to simply know that the test point is an area where all neighbors are positive, its exact identity is irrelevant.
 
 Decision trees are exploiting exactly that. Here, we do not store the training data, instead we use the training data to build a tree structure that recursively divides the space into regions with similar labels. The root node of the tree represents the entire data set. This set is then split roughly in half along one dimension by a simple threshold t
  All points that have a feature value  fall into the right child node, all the others into the left child node. The threshold t
 
 and the dimension are chosen so that the resulting child nodes are purer in terms of class membership. Ideally all positive points fall into one child node and all negative points in the other. If this is the case, the tree is done. If not, the leaf nodes are again split until eventually all leaves are pure (i.e. all its data points contain the same label) or cannot be split any further (in the rare case with two identical points of different labels).
 
 Decision trees have several nice advantages over nearest neighbor algorithms:
 
 \begin{enumerate}
 
 \item  once the tree is constructed, the training data does not need to be stored. Instead, we can simply store how many points of each label ended up in each leaf - typically these are pure so we just have to store the label of all points; \item  decision trees are very fast during test time, as test inputs simply need to traverse down the tree to a leaf - the prediction is the majority label of the leaf; \item  decision trees require no metric because the splits are based on feature thresholds and not distances.
 
  \end{enumerate}
 
 
 What we try to achieve is a maximally compact tree which has only pure leaves.It's always possible to make trees with pure leaves unless there are two different input vectors having identical features but different label.But it turns out that finding such minimum size tree is NP-hard(comlexity).We can approximate it very effectively with a greedy, top-down, Recursive partitioning approach.
 We keep splitting the data to minimize an impurity function that measures label purity amongst the children.There are different measures of impurity.
 
 
 
 \subsection{Impurity Functions}
 Data : $S = \{(x^{(1)},y^{(1)}),...,(x^{(n)},y^{(n)})\} \ , \ y^{(i)} \in \{1,2,...,c\},$ where $c$ is the number of classes. Let $S_{k} = \{(x,y) \in S : y  = k\}$, $S = S_{1} \cup ... \cup S_{c}.$ We define $P_{k} = \frac{\vert S_{k} \vert}{\vert S \vert}$ \ (fraction of inputs in S with label k) \\ 
 
\textbf{Gini impurity} of a leaf is defined as $G(S) = \sum_{k = 1}^{c} p_{k}(1 - p_{k})$.
Gini Impurity of a tree  : $G^{T}(S) = \frac{\vert S_{L} \vert}{\vert S \vert} G^{T}(S_{L}) + \frac{\vert S_{R} \vert}{\vert S \vert} G^{T}(S_{R})$
Where $S = S_{L} \cup S_{R}$;
$S_{L} \cap S_{R}  = \phi$;$\frac{\vert S_{L} \vert}{\vert S \vert} $ is fraction of inputs in the left sub tree.$\frac{\vert S_{R} \vert}{\vert S \vert}$ is fraction of inputs in the left subtree 

\subparagraph{Entropy}
 
 Entropy of a leaf $ H(s) = - \sum_{k} p_{k}log(p_k)$. Entropy over tree
 $H(s) = P^{L}H(S^{L}) + p^{R}H(S^R)$.Entropy of child nodes formed is weighted average of the nodes.The optimal split is the one in which entropy of the children is less than the parent.
 
 
 \begin{itemize}
 	\item How to find the tree with minimum entropy? 
 	\item How to find the optimal split? \ NP-Hard Problem
 	\item How to split? \ Try all splits.Take the one with lowest entropy
 	\item How many possible splits? \ For N data points, $x \in \mathbb{R}^{d}$ there are $(N-1)D$ possible splits.
 \end{itemize}
 
 
 
 
 
 
 
 
 
 
 \subsection{ID3-Algorithm}
 Base Cases: \\
 \[
 \textrm{ID3}(S):\left\{ \begin{array}{ll}
 \textrm{if } \exists \bar{y}\textrm{ s.t. }\forall(x,y)\in S, y=\bar{y}\Rightarrow \textrm{return leaf } \textrm{ with label } \bar{ y}\\
 \textrm{if } \exists\bar{x}\textrm{ s.t. }\forall(x,y)\in S, x=\bar{x}\Rightarrow \textrm{return leaf } \\  \ \ \ \ \textrm{ with mode}(y:(x,y)\in S)\textrm{ or mean (regression)}\end{array} \right.
 \]
 The Equation above indicates the ID3 algorithm stop under two cases. The first case is that all the data points in a subset of have the same label. If this happens, we should stop splitting the subset and create a leaf with label $y$. The other case is there are no more attributes could be used to split the subset. Then we create a leaf and label it with the most common $y$.
Try all features and all possible splits. Pick the split that minimizes impurity $(\textrm{e.g. } s>t)$ where $f\leftarrow$feature and $t\leftarrow$threshold
 
 Recursion: \\
 \[\textrm{Define: }\begin{bmatrix}
 S^L=\left \{ (x,y)\in S: x_f\leq t \right \}\\ 
 S^R=\left \{ (x,y)\in S: x_f> t \right \}
 \end{bmatrix}\]
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 \section{Bagging}
 
 Bagging is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting of any classifier. \\ Bias/Varience decomposition :
 $$\underbrace{\mathbb{E}[(h_{D}(x)-y)^{2}]}_\text{Error} = \underbrace{\mathbb{E}[(h_{D}(x) - \bar{h}(x))^{2}]}_\text{Variance} + \underbrace{\mathbb{E}[(\bar{h}(x) - \bar{y}(x))^{2}] }_\text{Bias} + \underbrace{\mathbb{E}[(\bar{y}(x)- y(x))^{2}]}_\text{Noise} $$
 
 
We would like to reduce the variance: ${\mathbb{E}[(h_{D}(x) - \bar{h}(x))^{2}]}$, for this to happen $h_{D} ] \mapsto \bar{h}$.The weak law of large numbers says that for independent and identically distributed $x_{i}$ with mean $\bar{x}$,
 
 $$\frac{1}{m} \sum_{i=1}^{m}x_{i} \rightarrow \bar{x} \ as \ m \ \rightarrow \infty$$
 
Assume that we have m taining sets $D_{1}, D_{2}, \dots D_{m}$ drawn from $P^{n}$.Train a classifier on each of the datset to obtain $h_{D_{i}}$s and extrapolating above idea to classifiers, we obtain  average of all such $h_{D_{i}}$s:

$$\hat{h} = \frac{1}{m} \sum_{i=1}^{m}h_{D_{i}} \rightarrow \bar{h} \ as \ m \rightarrow \infty$$.We refer to such an average of multiple classifiers as an ensemble of classifiers.If $\hat{h} \rightarrow \bar{h} \implies {\mathbb{E}[(h_{D}(x) - \bar{h}(x))^{2}]} \rightarrow 0$. But, as we saw earlier this needs m datsets, whereas we have only one dataset D.We can overcome this problem using \textbf{Bagging}(Bootstrap Aggregating)
 
 Simulate drawing  uniformly with replacement from the set D. i.e let $Q(X,Y\vert D)$ be a probability distribution that picks a training sample $(x_{i},y_{i})$ from $D$ uniformly at random.  More formally, $Q((x_{i},y_{i}) \vert D) = \frac{1}{n}  \ \forall (x_{i},y_{i}) \in D$ with $n = \vert D \vert$.We sample the set $D_{i} \tilde Q^{n}$, i.e $\vert D_{i} \vert = $ and $D_{i}$ is picked with replacement from $Q\vert D$.The bagged classifier is $\hat{h_{D}} = \frac{1}{m} \sum_{i=1}^{m}h_{D_{i}}$.Bagging doesnt imply $\hat{h} \rightarrow \bar{h}$ as Weak Law of Large Numbers doesnt apply here( W.L.L.N only works for i.i.d. samples).However, in practice bagging still reduces variance very effectively. 
 
 
 Although we cannot prove that the new samples are i.i.d., we can show that they are drawn from the original distribution P. Assume P is discrete, with $P(X=xi)=pi$ over some set (N very large) (let's ignore the label for now for simplicity)
 
 
 
 
 \section{Random Forest}
 
 Decision trees as such are not great classifiers because of bias-variance problem.The high variance problem of Decision trees can be reduced by using Bagging. One of the most famous and useful bagged algorithms is the Random Forest! A Random Forest is essentially nothing else but bagged decision trees, with a slightly modified splitting criteria.  
 
 
 \begin{enumerate}
 	\item Sample m data sets $D_{1},...,D_{m}$ from D with replacement.
 	\item  For each $D_{j}$ train a full decision tree $h_{j}()$ $(max-depth \ = \infty)$ with one small modification: before each split randomly subsample $k \geq d$ features (without replacement) and only consider these for your split. (This further increases the variance of the trees.)
 	
 	\item The Final Classifier is $h(x) = \frac{1}{m}\sum_{i=1}^{m}h_{j}(x)$ 
 \end{enumerate}
 
 The hyperparameters involved in a random forest are $m$ and $k$ A good choice for $k$ is $k = \sqrt{d}$ where d denotes the number of features.We can set m as large as you can afford. 
 
 
 
 
 
 
 
 
 \section{Kernels}
 
 Linear classifiers are great, but what if there exists no linear decision boundary? As it turns out, there is an elegant way to incorporate non-linearities into most linear classifiers.
 
 
 \textbf{ Feature Expansion}:We can make linear classifiers non-linear by applying basis function (feature transformations) on the input feature vectors. Formally, for a data vector $\mathbf{x}\in\mathbb{R}^d$, we apply the transformation $\mathbf{x} \rightarrow \phi(\mathbf{x})$ where $\phi(\mathbf{x})\in\mathbb{R}^D$. Usually $D \gg d$ because we add dimensions that capture non-linear interactions among the original features.Even after feature expansion the problem stays convex and well behaved. (i.e. you can still use your original gradient descent code, just with the higher dimensional representation) but $\phi(\mathbf{x})$ might be very high dimensional. 
 
 Consider the following example: \\ $\mathbf{x}=\begin{pmatrix}x_1\\ x_2\\ \vdots \\ x_d \end{pmatrix}$, and define $\phi(\mathbf{x})=\begin{pmatrix}1\\ x_1\\ \vdots \\x_d \\ x_1x_2 \\ \vdots \\ x_{d-1}x_d\\ \vdots \\x_1x_2\cdots x_d \end{pmatrix}$.
 
In all elements of $\phi(\mathbf{x})$, there are ${d \choose 0}$ zero-degree monomials, ${d \choose 1}$ one-degree monomials, ..., and ${d \choose d}$. As a sum-up, ${d \choose 0} +  {d \choose 1} +  {d \choose 2} + \cdots +  {d \choose d} =2^d$.Each element of $\phi(\mathbf{x})$ is equivalent to a subset of $\{x_1,\cdots,x_d\}$. A subset of $\{x_1,\cdots,x_d\}$ is determined by $d$ binary decisions: whether $x_i$ ($i=1\cdots d$) is in the subset of not. There are totally $2^d$ such decisions by combination, hence $\{x_1,\cdots,x_d\}$ has $2^d$ subsets and $\phi(\mathbf{x})$ has $2^d$ elements.
This new representation, $\phi(\mathbf{x})$, is very expressive and allows for complicated non-linear decision boundaries - but the dimensionality is extremely high. This makes our algorithm unbearable (and quickly prohibitively) slow.
 
\subsection{ The Kernel Trick}
 \textbf{Gradient Descent with Squared Loss}
 The kernel trick is a way to get around this dilemma by learning a function in the much higher dimensional space, without ever computing a single vector $\phi(\mathbf{x})$ or ever computing the full vector $\mathbf{w}$. It is a little magical. 
 
 It is based on the following observation: If we use gradient descent with any one of our standard loss functions the gradient is a linear combination of the input samples. For example, let us take a look at the squared loss: 
 
 \begin{equation}
 \ell(\mathbf{w}) = \sum_{i=1}^n (\mathbf{w}^\top  \mathbf{x}_i-y_i)^2\label{eq:c15:sql}
 \end{equation}
 The gradient descent rule, with step-size/learning-rate $s>0$ (we denoted this as $\alpha>0$ in our <a href="lecturenote10.html">previous lectures</a>), updates $\mathbf{w}$ over time,
 \begin{equation}
 w_{t+1} \leftarrow w_t - s(\frac{\partial \ell}{\partial \mathbf{w}})\ \textrm{ where: }
 \frac{\partial \ell}{\partial \mathbf{w}}=\sum_{i=1}^n \underbrace{2(\mathbf{w}^\top  \mathbf{x}_i-y_i)}_{\gamma_i\ :\ \textrm{function of $\mathbf{x}_i, y_i$}} \mathbf{x}_i = \sum_{i=1}^n\gamma_i \mathbf{x}_i
 \end{equation}
 
 We will now show that we can express $\mathbf{w}$ as a linear combination of all input vectors,
 \begin{equation}
 \mathbf{w}=\sum_{i=1}^n \alpha_i {\mathbf{x}}_i.\label{eq:c15:alphas}
 \end{equation}
 Since the loss is convex, the final solution is independent of the initialization, and we can initialize $\mathbf{w}^0$ to be whatever we want. For convenience, let us pick $\mathbf{w}_0=\begin{pmatrix}0 \\ \vdots \\ 0\end{pmatrix}$.  
 For this initial choice of $\mathbf{w}_0$, the linear combination in $\mathbf{w}=\sum_{i=1}^n \alpha_i {\mathbf{x}}_i$ is trivially  $\alpha_1=\dots=\alpha_n=0$. We now show that throughout the entire gradient descent optimization such coefficients $\alpha_1,\dots,\alpha_n$ must always exist, as we can re-write the gradient updates  entirely in terms of updating the $\alpha_i$  coefficients:
 \begin{align}
 \mathbf{w}_1=&\mathbf{w}_0-s\sum_{i=1}^n2(\mathbf{w}_0^\top  \mathbf{x}_i-y_i)\mathbf{x}_i=\sum_{i=1}^n \alpha_i^0 {\mathbf{x}}_i-s\sum_{i=1}^n\gamma_i^0\mathbf{x}_i=\sum_{i=1}^n\alpha_i^1\mathbf{x}_i  \ \ (\textrm{with $\alpha_i^1=\alpha_i^0-s\gamma_i^0$})\nonumber\\
 \mathbf{w}_2=&\mathbf{w}_1-s\sum_{i=1}^n2(\mathbf{w}_1^\top  \mathbf{x}_i-y_i)\mathbf{x}_i=\sum_{i=1}^n \alpha_i^1\mathbf{x}_i-s\sum_{i=1}^n\gamma_i^1\mathbf{x}_i=\sum_{i=1}^n\alpha_i^2\mathbf{x}_i \ \ (\textrm{with $\alpha_i^2=\alpha_i^1\mathbf{x}_i-s\gamma_i^1$})\nonumber\\
 \mathbf{w}_3=&\mathbf{w}_2-s\sum_{i=1}^n2(\mathbf{w}_2^\top  \mathbf{x}_i-y_i)\mathbf{x}_i=\sum_{i=1}^n \alpha_i^2\mathbf{x}_i-s\sum_{i=1}^n\gamma_i^2\mathbf{x}_i=\sum_{i=1}^n\alpha_i^3\mathbf{x}_i \ \ (\textrm{with $\alpha_i^3=\alpha_i^2-s\gamma_i^2$})\nonumber\\
 \cdots & \qquad\qquad\qquad\cdots &\cdots\nonumber\\
 \mathbf{w}_t=&\mathbf{w}_{t-1}-s\sum_{i=1}^n2(\mathbf{w}_{t-1}^\top  \mathbf{x}_i-y_i)\mathbf{x}_i=\sum_{i=1}^n \alpha_i^{t-1}\mathbf{x}_i-s\sum_{i=1}^n\gamma_i^{t-1}\mathbf{x}_i=\sum_{i=1}^n\alpha_i^t\mathbf{x}_i \  (\textrm{with $\alpha_i^t=\alpha_i^{t-1}-s\gamma_i^{t-1}$})\nonumber
 \end{align}
 
 Formally, the argument is by induction. $\mathbf{w}$ is trivially a linear combination of our training vectors for $\mathbf{w}_0$ (base case). If we apply the inductive hypothesis for $\mathbf{w}_t$ it follows for $\mathbf{w}_{t+1}$. 
 
 The update-rule for $\alpha_i^t$ is thus 
 \begin{equation}
 \alpha_i^t=\alpha_i^{t-1}-s\gamma_i^{t-1}, \textrm{ and we have } \alpha_i^t=-s\sum_{r=0}^{t-1}\gamma_i^{r}.
 \end{equation}
 In other words, we can perform the entire gradient descent update rule without ever expressing $\mathbf{w}$ explicitly. We just keep track of the $n$ coefficients $\alpha_1,\dots,\alpha_n$. 
 
 Now that $\mathbf{w}$ can be written as a linear combination of the training set, we can also express the inner-product of $\mathbf{w}$ with any input ${\mathbf{x}}_i$ purely in terms of inner-products between training inputs: 
 \begin{equation}
 \mathbf{w}^\top {\mathbf{x}}_j=\sum_{i=1}^n \alpha_i {\mathbf{x}}_i^\top{\mathbf{x}}_j.\nonumber
 \end{equation}
 Consequently, we can also re-write the squared-loss from $\ell(\mathbf{w}) = \sum_{i=1}^n (\mathbf{w}^\top  \mathbf{x}_i-y_i)^2$ entirely in terms of inner-product between training inputs:
 \begin{equation}
 \ell(\mathbf{\alpha}) = \sum_{i=1}^n \left(\sum_{j=1}^n\alpha_j\mathbf{x}_j^\top  \mathbf{x}_i-y_i\right)^2\label{eq:c15:sql:ip} 
 \end{equation}
 During test-time we also only need these coefficients to make a prediction on a test-input $x_t$, and can write the entire classifier in terms of inner-products between the test point and training points:
 \begin{equation}
 h({\mathbf{x}}_t)=\mathbf{w}^\top {\mathbf{x}}_t=\sum_{j=1}^n\alpha_j{\mathbf{x}}_j^\top {\mathbf{x}}_t.
 \end{equation}
 Do you notice a theme? The only information we ever need in order to learn a hyper-plane classifier with the squared-loss is inner-products between all pairs of data vectors. 
 
 \subsection{Inner Product Computation}
 
 Let's go back to the previous example, $\phi(\mathbf{x})=\begin{pmatrix}1\\ x_1\\ \vdots \\x_d \\ x_1x_2 \\ \vdots \\ x_{d-1}x_d\\ \vdots \\x_1x_2\cdots x_d \end{pmatrix}$. 
 
 
 The inner product $\phi(\mathbf{x})^\top \phi(\mathbf{z})$ can be formulated as: 
 \begin{equation}
 \phi(\mathbf{x})^\top \phi(\mathbf{z})=1\cdot 1+x_1z_1+x_2z_2+\cdots +x_1x_2z_1z_2+ \cdots +x_1\cdots x_dz_1\cdots z_d=\prod_{k=1}^d(1+x_kz_k)\text{.}\label{eq:c15:poly}
 \end{equation}
 The sum of $2^d$ terms becomes the product of $d$ terms. We can compute the inner-product from the above formula in time $O(d)$ instead of $O(2^d)$! 
 We define the function 
 \begin{equation}
 \underbrace{\mathsf{k}(\mathbf{x}_i,\mathbf{x}_j)}_{\text{this is called the} \textbf{ kernel function}}=\phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j).
 \end{equation}
 With a finite training set of $n$ samples, inner products are often pre-computed and stored in a Kernel Matrix:
 \begin{equation}
 \mathsf{K}_{ij}=\phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j).
 \end{equation}
 If we store the matrix $\mathsf{K}$, we only need to do simple inner-product look-ups and low-dimensional computations throughout the gradient descent algorithm. 
 The final classifier becomes:
 \begin{equation}
 h(\mathbf{x}_t)=\sum_{j=1}^n\alpha_j\mathsf{k}(\mathbf{x}_j,\mathbf{x}_t).
 \end{equation}
 
 During training in the new high dimensional space of $\phi(\mathbf{x})$ we want to compute $\gamma_i$ through kernels, without ever computing any $\phi(\mathbf{x}_i)$ or even $\mathbf{w}$. We previously established that $\mathbf{w}=\sum_{j=1}^n\alpha_j \phi(\mathbf{x}_j)$, and 
 $\gamma_i=2(\mathbf{w}^\top \phi(\mathbf{x}_i)-y_i)$. It follows that $\gamma_i=2(\sum_{j=1}^n \alpha_jK_{ij})-y_i)$. The gradient update in iteration $t+1$ becomes $$\alpha_i^{t+1}\leftarrow \alpha_i^t-2s(\sum_{j=1}^n \alpha_j^tK_{ij})-y_i).$$
 As we have $n$ such updates to do, the amount of work per gradient update in the transformed space is $O(n^2)$ --- far better than $O(2^d)$. 
 
 
 \subsection{General Kernels}
 
 Below are some popular kernel functions: 
 \begin{itemize}
\item Linear: $\mathsf{K}(\mathbf{x},\mathbf{z})=\mathbf{x}^\top \mathbf{z}$.
 
 (The linear kernel is equivalent to just using a good old linear classifier - but it can be faster to use a kernel matrix if the dimensionality $d$ of the data is high.)
 
 \item Polynomial: $\mathsf{K}(\mathbf{x},\mathbf{z})=(1+\mathbf{x}^\top \mathbf{z})^d$.
 
 \item Radial Basis Function (RBF) (aka Gaussian Kernel): $\mathsf{K}(\mathbf{x},\mathbf{z})= e^\frac{-\|\mathbf{x}-\mathbf{z}\|^2}{\sigma^2}$.
 
 The RBF kernel is the most popular Kernel! It is a \href{"https://en.wikipedia.org/wiki/Universal_approximation_theorem}{Universal approximator}. Its corresponding feature vector is infinite dimensional and cannot be computed. However, very effective low dimensional approximations exist.
  RBF is universal approximator, but we dont  use it all the time, because if our data set size $n$ is very large, the $n\times n$ kernel matrix can become too large and too expensive to compute.
 
 
 \item Exponential Kernel: $\mathsf{K}(\mathbf{x},\mathbf{z})= e^\frac{-\| \mathbf{x}-\mathbf{z}\|}{2\sigma^2}$
 
\item  Laplacian Kernel: $\mathsf{K}(\mathbf{x},\mathbf{z})= e^\frac{-| \mathbf{x}-\mathbf{z}|}{\sigma}$
 
 \item Sigmoid Kernel: $\mathsf{K}(\mathbf{x},\mathbf{z})=\tanh(\mathbf{a}\mathbf{x}^\top  + c)$
 
\end{itemize}
 
  
  \subsection{Kernel Functions}
  
Can any function $\mathsf{K}(\cdot,\cdot)\rightarrow{\mathcal{R}}$ be used as a kernel?No, the matrix $\mathsf{K}(\mathbf{x}_i,\mathbf{x}_j)$ has to correspond to real inner-products after some transformation ${\mathbf{x}}\rightarrow \phi({\mathbf{x}})$. This is the case if and only if $\mathsf{K}$ is positive semi-definite. A matrix $A\in \mathbb{R}^{n\times n}$ is positive semi-definite iff $\forall \mathbf{q}\in\mathbb{R}^n$, $\mathbf{q}^\top A\mathbf{q}\geq 0$.
Remember $\mathsf{K}_{ij}=\phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j)$. So $\mathsf{K}=\Phi^\top\Phi$, where $\Phi=[\phi(\mathbf{x}_1),\dots,\phi(\mathbf{x}_n)]$.
It follows that $\mathsf{K}$ is p.s.d., because $\mathbf{q}^\top\mathsf{K}\mathbf{q}=(\Phi^\top \mathbf{q})^2\geq 0$. Inversely, if any matrix $\mathbf{A}$ is p.s.d., it can be decomposed as $A=\Phi^\top\Phi$ for some realization of $\Phi$. \\
A matrix of form $\mathsf{K}=\begin{pmatrix} \mathbf{x}_1^\top \mathbf{x}_1, ..., \mathbf{x}_1^\top \mathbf{x}_n \\ \vdots ~ \vdots \\ \mathbf{x}_n^\top \mathbf{x}_1, ..., \mathbf{x}_n^\top \mathbf{x}_n \end{pmatrix}=\begin{pmatrix} \mathbf{x}_1\\ \vdots \\ \mathbf{x}_n \end{pmatrix} \begin{pmatrix} \mathbf{x}_1, \cdots \mathbf{x}_n \end{pmatrix}$  must be positive semi-definite because:
 $\mathbf{q}^\top A\mathbf{q}=(\underbrace{\begin{pmatrix} \mathbf{x}_1, \cdots \mathbf{x}_n \end{pmatrix}\mathbf{q}}_{\text{a vector with the same dimension of } \mathbf{x}_i})^\top (\begin{pmatrix} \mathbf{x}_1, \cdots \mathbf{x}_n \end{pmatrix}\mathbf{q})\geq 0$ for $\forall \mathbf{q}\in\mathbb{R}^n$. 
 
 
 
 \section{Clustering}
 Clustering is an unsupervised learning problem.
 
 
 \subsection{K-means Optimization Problem}
 
 \begin{itemize}
 	\item Input : $x_{1}, x_{2}, ... , x_{n} \in \mathbb{R}^{d} \ ; integer \ k$
 	\item Output : Centers or representatives $\mu_{1},...,\mu_{k} \in \mathbb{R} ^{d}$
 	\item Goal: minimize average squared distance between points and their nearest representative  $ cost(\mu_{1},...,\mu_{k}) = \sum_{i=1}^{n} \vert \vert x_{i} - \mu_{j} \vert \vert ^{2} $
 \end{itemize}
The centers partition $\mathbb{R}^{d}$ into k- convex regions.$\mu_{j}'$s region consist of points for which it is the closest center.



 \begin{algorithm}
 	
 	\caption*{Lloyd's k-means Algorithm} \label{alg:MyAlgorithm}
 	\begin{algorithmic}
 		
 		\STATE 1. \ Initialize cluster centers $\mu_{1},...,\mu_{k}$ in some manner.
 		\STATE 2. \ Repeat until convergence:
 		
 		
 		
 		
 		
 		\STATE \hspace{ 0.8cm} For every $i$, set  \\  \hspace{4cm}$c^{i} := arg \min_{j}   \vert \vert x_{i} - \mu_{j} \vert \vert ^{2} $
 		
 		\STATE \hspace{ 0.8cm} For each j, set \\  \hspace{4 cm}$\mu_{j} = \frac{\sum_{i=1}^{m} 1 \{c^{(i)}=j\}x^{(i)}}{\sum_{i=1}^{m} 1 \{c^{(i)}=j\}}$
 		
 		
 		
 		
 		
 	\end{algorithmic}
 \end{algorithm}
 
 
 
In the iteration, $(1)$ refers to assigning each point to closest center,$(2)$ refers to updating each $\mu_{j}$ to the mean of the points assigned to it.Initializing the cluster centroids is usually done by choosing $k$ training examples randomly
 
 
  

 \subsection{Clustering with mixtures of Gaussian}
 
Given: $x_{1}$, $x_{2}$, ... ,$x_{n}$ $\in \mathbb R^{d}$.We need to clusterize the data.\\Each of the k-clusters is defined by:
\begin{itemize}
	\item A gaussian distribution $P_{j}=N(\mu_{j},\Sigma_{j})$ , $\mu_{j} \in \mathbb R^{d}$ , $\Sigma_{j} \in \mathbb R^{dxd}$ 
     \item	A mixing weight $\pi_{j}$
\end{itemize}
$Pr(x)=\sum_{j}Pr(cluster)Pr(x|cluster \ j)$
The Overall distribution over $\mathbb R^{d}$ is a mixture of Gaussian i.e $Pr(x)=\pi_{1}P_{1}(x)+\pi_{2}P_{2}(x)+...+\pi_{k}P_{k}(x)$\\ \textbf{The Clustering Task:}\\
\begin{itemize}
	\item Given: $x_{1}$, $x_{2}$, ... ,$x_{n}$ $\in \mathbb R^{d}$
	\item For any mixture model $\pi_{1}$, $\pi_{2}$,...,$\pi_{k}$ and $P_{1}=N(\mu_{1},\Sigma_{1})$,...,$P_{k}=N(\mu_{k},\Sigma_{k})$
	
\item	$\mathrm{Pr}(\mathrm{data} \vert \sum_{i=1}^{k}\pi_{i} \mathrm{P_{i}})=\underset{i}\Pi$ $ \mathrm{Pr(x_{i}})$ $ =\underset{i=1 }   \Pi \overset{n}  (Pr(x)=\pi_{1}P_{1}(x)+\pi_{2}P_{2}(x)+...+\pi_{k}P_{k}(x))=$ $\underset{i=1}\Pi $  $\underset{j=1} \sum  \pi_{j}\mathrm{P_{j(x_{i})}} $
	
	
 \item	$\mathrm{Pr}(\mathrm{data} \vert \sum_{i=1}^{k}\pi_{i} \mathrm{P_{i}})$
	$=\prod_{i=1}^{n}\{\sum_{j=1}^{k}\frac{\pi_{j}}{(2\pi)^{d/2} \vert \Sigma_{j}\vert^{\frac{1}{2}}  } 
	\exp (\frac{-1}{2} (x_{i}-\mu_{j})^{T}\Sigma_{j}^{-1}(x_{i}-\mu_{j}))\}$
	
	\item This is the likelihhod of the data under the model $\pi_{1}$, $\pi_{2}$,...,$\pi_{k}$ and $P_{1}=N(\mu_{1},\Sigma_{1})$,...,$P_{k}=N(\mu_{k},\Sigma_{k})$
	
	
	Now the task is to find the maximum likelihood mixture of gaussians i.e to find the parameters $\{\pi_{j},\mu_{j},\Sigma_{j}:j=1,...,k\}$ that maximizes the function
	
\item	Maximizing $\mathrm{Pr}(\mathrm{X} \vert \sum_{i=1}^{k}\pi_{i} \mathrm{P_{i}})$  is same as maximizing log of the same maximizing $\log \mathrm{Pr}(\mathrm{X} \vert \sum_{i=1}^{k}\pi_{i} \mathrm{P_{i}})$ is equivalent to \item minimizing neagative $\log \mathrm{Pr}(\mathrm{X} \vert \sum_{i=1}^{k}\pi_{i} \mathrm{P_{i}})$ 
\item Minimizing the negative log-likelihood \\
	$\mathrm{L}(\{\pi_{j},\mu_{j},\Sigma_{j} \})=\sum_{i=1}^{n} \ln \{\sum_{j=1}^{k}\frac{\pi_{j}}{(2\pi)^{d/2} \vert \Sigma_{j}\vert^{\frac{1}{2}}  } 
	\exp (\frac{-1}{2} (x_{i}-\mu_{j})^{T}\Sigma_{j}^{-1}(x_{i}-\mu_{j}))\} $
	
	\item L is not a convex function,have multiple local minima.Finding global minima is a NP hard problem
\end{itemize}
 
\textbf{Expectation Maximization (EM) Algorithm}

\begin{itemize}
	\item Initialize $\pi_{1}$, $\pi_{2}$,...,$\pi_{k}$ and $P_{1}=N(\mu_{1},\Sigma_{1})$,...,$P_{k}=N(\mu_{k},\Sigma_{k})$
	
\item	Repeat until convergence:
	Assign each point $x_{i}$ fractionally between k-clusters 
	$\mathrm{w_{ij}} = \mathrm{Pr}(\mathrm{cluster,j} \vert x_{i})=\frac {\pi_{j}P_{j}(x_{i})} {\sum_{l}P_{l}(x_{i})}$
	
\item	Update mixing weights,means,covariances
	$\pi_{j}=\frac{1}{n}\sum_{i=1}^{n}w_{ij}$
	$\mu_{j}=\frac{1}{n\pi_{j}}\sum_{i=1}^{n}w_{ij}x_{i}$
	$\Sigma_{j}=\frac{1}{n\pi_{j}} \sum_{i=1}^{n}w_{ij}(x_{i}-\mu_{j})(x_{i}-\mu_{j})^{T}$
\end{itemize}
 


\section{Generative Approch to Classification}

The Learning Problem - Fit a probability distribution to each class individually.To classify a new point - Which of the distribution was it most likely to have come from?


\begin{itemize}
\item	For each class j,we have probability of that class, $\pi_{j}=Pr(y=j)$ and distribution of data in that class $P_{j}(x)$ 
	
	\item The Overall joint distribution is: $Pr(x,y)=Pr(y)Pr(x\vert y)=\pi_{y}P_{y}(x)$
	
	\item To classify a new x: pick the label y with largest $Pr(x,y)$
	
\end{itemize}

 
\textbf{Two Dimensional Genearative Modelling with Bivariate Gaussian}

\begin{itemize}
\item	Distribution over random varibales $x_{1}$ and $x_{2}$ ,$(x_{1} ,  x_{2})\in R^{2}$
	
\item	Mean $(\mu_{1},\mu _{2}) \in R^{2}$,  $ \mu_{1}=E(x_{1})$,  $\mu_{2}=E(x_{2})$
	
\item	covarince of two random variables $x_{1},x_{2}$ is : $cov(x_{1},x_{2})=E(x_{1}x_{2})-E(x_{1})E(x_{2})$,where $\sum_{11}=Var(x_{1})$, $\sum_{22}=Var(x_{2})$, $\sum_{21}=\sum_{12}=cov(x_{1},x_{2})$
	
\item	The density $P(x_{1},x_{2})$ is given by, $$P(x_{1},x_{2})=\frac{1}{2\pi \vert \sum \vert ^{\frac{1}{2}}} \exp[-\frac{1}{2} \left({\begin{array}{cc}{x_{1}-\mu_{1}}\\\ {x_{1}-\mu_{1}}\end{array}}\right) \sum^{-1} \left({\begin{array}{cc}{x_{1}-\mu_{1}}\\\ {x_{1}-\mu_{1}}\end{array}}\right)]$$
	
\item Density is highest at the mean $(\mu_{1},\mu_{2})$ and falls off in a ellipsoidal contours.The shape of the ellipsoid is determined by the covariance matrix
	

Find an implementation \href{https://github.com/aswin16/ML-REPORT/blob/master/codes/winery-multivariate/winery-classification-gaussian.ipynb}{here}
	
	
\end{itemize}



















 
\end{document}